{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7e0159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pygame, sys, pygame.locals\n",
    "import game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28dcfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(x1, y1, x2, y2, d1, d2, l1, l2):\n",
    "    distance = (x2 - x1) ** 2\n",
    "    distance += (y2 - y1) ** 2\n",
    "    distance = min((distance ** 0.5) / 25, 5)\n",
    "    \n",
    "    cdistance1 = min(((((x1 - 400) ** 2) + ((y1 - 225) ** 2)) ** 0.5) / 50, 2)\n",
    "    cdistance2 = min(((((x2 - 400) ** 2) + ((y2 - 225) ** 2)) ** 0.5) / 50, 2)\n",
    "    \n",
    "    lit1 = -5 if l1 > 90 else 0\n",
    "    lit2 = -5 if l2 > 90 else 0\n",
    "    facing1 = -5\n",
    "    \n",
    "    facing2 = -5\n",
    "    if d1 == 0 and x1 < x2:\n",
    "        facing1 = 10\n",
    "    elif d1 == 1 and x1 > x2:\n",
    "        facing1 = 10\n",
    "    elif d1 == 2 and y1 < y2:\n",
    "        facing1 = 5\n",
    "    elif d1 == 3 and y1 > y2:\n",
    "        facing1 = 5\n",
    "    \n",
    "    if d2 == 0 and x2 < x1:\n",
    "        facing2 = 10\n",
    "    elif d2 == 1 and x2 > x1:\n",
    "        facing2 = 10\n",
    "    elif d2 == 2 and y2 < y1:\n",
    "        facing2 = 5\n",
    "    elif d2 == 3 and y2 > y1:\n",
    "        facing2 = 5\n",
    "    return [\n",
    "        (30 - distance - cdistance1 + lit1 + facing1) / 10,\n",
    "        (30 - distance - cdistance2 + lit2 + facing2) / 10\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7515d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collision(player, spell):\n",
    "    if ((spell.x > player.x and spell.x < player.x + game.PS * 2) or (spell.x + spell.dx > player.x and spell.x + spell.dx < player.x + game.PS * 2)) and ((spell.y > player.y and spell.y < player.y + game.PS * 2) or (spell.y + spell.dy > player.y and spell.y + spell.dy < player.y + game.PS * 2)):\n",
    "        return True\n",
    "    if ((player.x > spell.x and player.x < spell.x + spell.dx) or (player.x + player.x + game.PS * 2 > spell.x and player.x + player.x + game.PS * 2 < spell.x + spell.dx)) and ((player.y > spell.y and player.y < spell.y + spell.dy) or (player.y + game.PS * 2 > spell.y and player.y + game.PS * 2 < spell.y + spell.dy)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class KCEnv():\n",
    "    metadata = {'render_modes' : ['human'], 'render_fps' : 30}\n",
    "    \n",
    "    def __init__(self, conv, moveset):\n",
    "        self.conv = conv\n",
    "        self.DISPLAY = pygame.display.set_mode((800, 450), flags = pygame.SCALED)\n",
    "        pygame.display.set_caption('Kinetic Chance')\n",
    "        self.players = [game.Player(200, 4, moveset, (0, 0, 255), 2, 80, 280), game.Player(200, 4, moveset, (255, 0, 0), 2, 720, 280)]\n",
    "        self.spells = []\n",
    "        pygame.init()\n",
    "        self.font = pygame.font.SysFont('Arial', 20)\n",
    "        self.moveset = moveset\n",
    "    def step(self, action):\n",
    "        rewards = reward_function(\n",
    "            self.players[0].x, self.players[0].y, \n",
    "            self.players[1].x, self.players[1].y,\n",
    "            self.players[0].dir, self.players[1].dir,\n",
    "            self.players[0].lit, self.players[1].lit\n",
    "        )\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        self.DISPLAY.fill((40, 40, 40))\n",
    "        self.render_text()\n",
    "        for i in range(len(self.players)):\n",
    "            if not self.players[i].alive:\n",
    "                continue\n",
    "            self.players[i].tick()\n",
    "            newspell = self.players[i].autocast()\n",
    "            if newspell != None:\n",
    "                self.spells.append(newspell)\n",
    "                self.spells[-1].owner = i\n",
    "            self.players[i].render(self.DISPLAY)\n",
    "            self.players[i].control(action[i][0])\n",
    "            self.players[i].control(action[i][1] + 5)\n",
    "            self.players[i].control(action[i][3] + 10)\n",
    "            newspell = self.players[i].control(action[i][2] + 8)\n",
    "            if newspell != None:\n",
    "                self.spells.append(newspell)\n",
    "                self.spells[-1].owner = i\n",
    "            self.players[i].move()\n",
    "            for j in range(len(self.spells)):\n",
    "                if self.spells[j] == -1:\n",
    "                    continue\n",
    "                if i != self.spells[j].owner:\n",
    "                    if collision(self.players[i], self.spells[j]):\n",
    "                        self.players[i].trigger(self.spells[j])\n",
    "                        o = self.spells[j].owner\n",
    "                        self.spells[j] = -1\n",
    "                        rewards[i] -= 1000\n",
    "                        rewards[o] += 1000\n",
    "                elif self.spells[j].movetype == 1:\n",
    "                    self.spells[j].init(self.players[i].dir, self.players[i].x, self.players[i].y)\n",
    "        for i in range(len(self.spells)):\n",
    "            if self.spells[i] == -1:\n",
    "                continue\n",
    "            self.spells[i].render(self.DISPLAY)\n",
    "            if self.spells[i].tick():\n",
    "                self.spells[i] = -1\n",
    "        while -1 in self.spells:\n",
    "            self.spells.remove(-1)\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        pygame.display.flip()\n",
    "        if self.conv:\n",
    "            self.observation = np.array(pygame.surfarray.array3d(self.DISPLAY), dtype = np.float32) / 255\n",
    "        else:\n",
    "            self.observation = np.array([\n",
    "                self.players[0].x / 800,\n",
    "                self.players[0].y / 450,\n",
    "                self.players[0].dir,\n",
    "                self.players[0].lit / 30,\n",
    "                self.players[0].streak / 2,\n",
    "                self.players[0].selmove / 5,\n",
    "                self.players[1].x / 800,\n",
    "                self.players[1].y / 450,\n",
    "                self.players[1].dir,\n",
    "                self.players[1].lit / 30,\n",
    "                self.players[1].streak / 2,\n",
    "                self.players[1].selmove / 5],\n",
    "                dtype = np.float32)\n",
    "        return self.observation, rewards, not (self.players[0].alive and self.players[1].alive), False, {}\n",
    "    def reset(self):\n",
    "        self.players = [\n",
    "            game.Player(200, 4, self.moveset, (0, 0, 255), 2, 80, 280), \n",
    "            game.Player(200, 4, self.moveset, (255, 0, 0), 2, 720, 280)]\n",
    "        self.players[0].render(self.DISPLAY)\n",
    "        self.players[1].render(self.DISPLAY)\n",
    "        self.spells = []\n",
    "        if self.conv:\n",
    "            self.observation = np.array(pygame.surfarray.array3d(self.DISPLAY), dtype = np.float32) / 255\n",
    "        else:\n",
    "            self.observation = np.array([\n",
    "                self.players[0].x / 800,\n",
    "                self.players[0].y / 450,\n",
    "                self.players[0].dir,\n",
    "                self.players[0].lit / 30,\n",
    "                self.players[0].streak / 2,\n",
    "                self.players[0].selmove / 5,\n",
    "                self.players[1].x / 800,\n",
    "                self.players[1].y / 450,\n",
    "                self.players[1].dir,\n",
    "                self.players[1].lit / 30,\n",
    "                self.players[1].streak / 2,\n",
    "                self.players[1].selmove / 5],\n",
    "                dtype = np.float32)\n",
    "        return self.observation, {}\n",
    "    def render_text(self):\n",
    "        self.DISPLAY.blit(self.font.render(\n",
    "        'Lit: {}, {} Moves: {}, {}'.format(\n",
    "            str(round(self.players[0].lit)), \n",
    "            str(round(self.players[1].lit)),\n",
    "            self.players[0].moveset[self.players[0].selmove],\n",
    "            self.players[1].moveset[self.players[1].selmove]),\n",
    "        True, (255, 255, 255)), (50, 50))\n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b675b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VecActor(nn.Module):\n",
    "    def __init__(self, actions):\n",
    "        super(VecActor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(12, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class VecCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VecCritic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(12, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7cc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvActor(nn.Module):\n",
    "    def __init__(self, actions):\n",
    "        super(ConvActor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(3, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(4, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(220, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, actions)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd97ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvCritic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(3, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(4, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(220, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200521d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, idims, alpha, gamma, eps, l, epochs, bsize, liters, conv):\n",
    "        self.actor = ConvActor(actions) if conv else VecActor(actions)\n",
    "        self.critic = ConvCritic() if conv else VecCritic()\n",
    "        self.asize = actions\n",
    "        self.idims = idims\n",
    "        self.actor.opt = torch.optim.Adam(self.actor.parameters(), lr = alpha)\n",
    "        self.critic.opt = torch.optim.Adam(self.critic.parameters(), lr = alpha)\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.l = l\n",
    "        self.epochs = epochs\n",
    "        self.bsize = bsize\n",
    "        self.liters = liters\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def reset_mem(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def store_mem(self, state, prob, action, reward, value, done):\n",
    "        self.states.append(state.tolist())\n",
    "        self.probs.append(prob)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def batching(self):\n",
    "        bsindices = np.arange(0, len(self.states), self.bsize)\n",
    "        bindices = np.arange(0, self.bsize, dtype = np.int64)\n",
    "        np.random.shuffle(bindices)\n",
    "        bindices = bindices.tolist()\n",
    "        rvalue = []\n",
    "        for i in bsindices:\n",
    "            rvalue += bindices[i:i + self.bsize]\n",
    "        return np.array(rvalue, dtype = np.int64)\n",
    "        \n",
    "    def act(self, obs):\n",
    "        obs = obs.unsqueeze(0)\n",
    "        policy = self.actor(obs).squeeze()\n",
    "        policy1 = Categorical(F.softmax(policy[0:5], dim = -1))\n",
    "        policy2 = Categorical(F.softmax(policy[5:8], dim = -1))\n",
    "        policy3 = Categorical(F.softmax(policy[8:10], dim = -1))\n",
    "        policy4 = Categorical(F.softmax(policy[10:12], dim = -1))\n",
    "        value = self.critic(obs).squeeze().item()\n",
    "        #action and log probs will be of size 3\n",
    "        action1 = policy1.sample()\n",
    "        action2 = policy2.sample()\n",
    "        action3 = policy3.sample()\n",
    "        action4 = policy4.sample()\n",
    "        #since these log probs are passed directly into store mem,\n",
    "        #and the same is done with the new probs, only the sum is returned\n",
    "        prob1 = policy1.log_prob(action1).item()\n",
    "        prob2 = policy2.log_prob(action2).item()\n",
    "        prob3 = policy3.log_prob(action3).item()\n",
    "        prob4 = policy3.log_prob(action3).item()\n",
    "        \n",
    "        return [action1.item(), action2.item(), action3.item(), action4.item()], prob1 + prob2 + prob3 + prob4, value\n",
    "        \n",
    "    def learn(self):\n",
    "        adv = np.zeros(len(self.rewards) - 1, dtype = np.float32)\n",
    "        self.states = np.array(self.states, dtype = np.float32)\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            bindices = np.array(self.batching(), dtype = np.int64)\n",
    "            #gae\n",
    "            #summation of memory\n",
    "            for j in range(len(self.rewards) - 1):\n",
    "                #delta coefficient\n",
    "                disc = 1\n",
    "                #advantage\n",
    "                a = 0\n",
    "                for k in range(j, len(self.rewards) - 1):\n",
    "                    #delta of timestep = (done coefficient * gamma * next state value) + reward - current state value\n",
    "                    #basically new value + reward - cur value\n",
    "                    a += disc * (((1 - self.dones[k]) * self.gamma * self.values[k + 1]) + self.rewards[k] - self.values[k])\n",
    "                    #gae lamba^n * gamma^n\n",
    "                    disc *= self.gamma * self.l\n",
    "                #adv at each timestep\n",
    "                adv[j] = a\n",
    "            adv = torch.Tensor(adv).float()\n",
    "            sb = []\n",
    "            p1 = []\n",
    "            ab = []\n",
    "            vb = []\n",
    "            advb = []\n",
    "            #sampling of random memory\n",
    "            for i in bindices:\n",
    "                sb.append(self.states[i])\n",
    "                p1.append(self.probs[i])\n",
    "                ab.append(self.actions[i])\n",
    "                vb.append(self.values[i])\n",
    "                advb.append(adv[i])\n",
    "            sb = torch.Tensor(np.array(sb)).float()\n",
    "            #these 2 are size 4 for the multi discrete implementation\n",
    "            p1 = torch.Tensor(np.array(p1)).float()\n",
    "            ab = torch.Tensor(np.array(ab)).long()\n",
    "            vb = torch.Tensor(np.array(vb)).float()\n",
    "            advb = torch.Tensor(advb).float()\n",
    "            #predictions\n",
    "            apred = self.actor(sb)\n",
    "            apred1 = Categorical(F.softmax(apred[0, 0:5], dim = -1))\n",
    "            apred2 = Categorical(F.softmax(apred[0, 5:8], dim = -1))\n",
    "            apred3 = Categorical(F.softmax(apred[0, 8:10], dim = -1))\n",
    "            apred4 = Categorical(F.softmax(apred[0, 10:12], dim = -1))\n",
    "            cpred = self.critic(sb)\n",
    "            #get new log probs corresponding to past actions from memory\n",
    "            #there are 3 of these now\n",
    "            #in the 37 implementation details thingy, they multiplied the probs for each distribution\n",
    "            #since these are logits, they shall be added\n",
    "            p2 = apred1.log_prob(ab[:, 0]) + apred2.log_prob(ab[:, 1]) + apred3.log_prob(ab[:, 2]) + apred4.log_prob(ab[:, 3])\n",
    "            #actor loss calculation: this is the same now that the probs are combined\n",
    "            pratio = p2.exp() / p1.exp()\n",
    "            wpratio = pratio * advb\n",
    "            cwpratio = torch.clamp(pratio, 1 - self.eps, 1 + self.eps) * advb\n",
    "            aloss = (-torch.min(wpratio, cwpratio)).mean()\n",
    "            #critic loss: gae + state value MSE'd with raw network prediction\n",
    "            #gae + state value = new state + reward\n",
    "            #in other words, optimize state value to become new state + reward\n",
    "            ctarget = advb + vb\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            #closs = ((ctarget - cpred) ** 2).mean()\n",
    "            closs = criterion(ctarget.unsqueeze(-1), cpred)\n",
    "            #now includes entropy term\n",
    "            entropy = (0.1 * apred1.entropy()) + (0.8 * apred2.entropy()) + (0.2 * apred3.entropy()) + (0.1 * apred4.entropy())\n",
    "            loss = aloss + (0.5 * closs) - (0.4 * entropy)\n",
    "            self.actor.opt.zero_grad()\n",
    "            self.critic.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.actor.opt.step()\n",
    "            self.critic.opt.step()\n",
    "        self.reset_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f291dd37",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m     nobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(nobs)\n\u001b[0;32m     48\u001b[0m agent1\u001b[38;5;241m.\u001b[39mstore_mem(obs, prob1, action1, rewards[\u001b[38;5;241m0\u001b[39m], value1, done)\n\u001b[1;32m---> 49\u001b[0m \u001b[43magent2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_mem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m score1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     51\u001b[0m score2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m, in \u001b[0;36mAgent.store_mem\u001b[1;34m(self, state, prob, action, reward, value, done)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore_mem\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, prob, action, reward, value, done):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(state\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mappend(prob)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "ACTIONS = 12\n",
    "MOVESET = ['flare', 'multisword', 'heal', 'intoxicate', 'cannon']\n",
    "INPUT_DIMS = 4\n",
    "LR = 5e-4\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "POLICY_CLIP = 0.1\n",
    "SMOOTHING = 0.95\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 5\n",
    "LEARN_ITERS = 20\n",
    "CONV = False\n",
    "\n",
    "env = KCEnv(CONV, MOVESET)\n",
    "\n",
    "agent1 = Agent(ACTIONS, INPUT_DIMS, LR, DISCOUNT_FACTOR, POLICY_CLIP, SMOOTHING, EPOCHS, BATCH_SIZE, LEARN_ITERS, CONV)\n",
    "agent2 = Agent(ACTIONS, INPUT_DIMS, LR, DISCOUNT_FACTOR, POLICY_CLIP, SMOOTHING, EPOCHS, BATCH_SIZE, LEARN_ITERS, CONV)\n",
    "\n",
    "agent1 = torch.load('vecagent1A.pt')\n",
    "agent2 = torch.load('vecagent2A.pt')\n",
    "\n",
    "EPISODES = 5\n",
    "\n",
    "steps = 0\n",
    "nobs = 0\n",
    "rewards = [0, 0]\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    \n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.locals.QUIT:\n",
    "          pygame.quit()\n",
    "          sys.exit()\n",
    "    \n",
    "    obs = torch.tensor(env.reset()[0])\n",
    "    if CONV:\n",
    "        obs = obs.transpose(0, -1)\n",
    "    done = False\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    while not done:\n",
    "        action1, prob1, value1 = agent1.act(obs)\n",
    "        action2, prob2, value2 = agent2.act(obs)\n",
    "        nobs, rewards, done, _, _ = env.step([action1, action2])\n",
    "        if CONV:\n",
    "            nobs = torch.tensor(nobs).transpose(0, -1)\n",
    "        else:\n",
    "            nobs = torch.tensor(nobs)\n",
    "        agent1.store_mem(obs, prob1, action1, rewards[0], value1, done)\n",
    "        agent2.store_mem(obs, prob2, action2, rewards[1], value2, done)\n",
    "        score1 += rewards[0] - 3\n",
    "        score2 += rewards[1] - 3\n",
    "        steps += 1\n",
    "        obs = nobs\n",
    "        if steps % agent1.liters == 0:\n",
    "            agent1.learn()\n",
    "            agent2.learn()\n",
    "        pygame.time.Clock().tick(144)\n",
    "    print(\"Episode: {} Scores: {}, {}\".format(i + 1, score1, score2))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa05ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76917c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent1, 'vecagent1A.pt')\n",
    "torch.save(agent2, 'vecagent2A.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
